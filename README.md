## README

### Обзор

Этот проект предназначен для реализации и оценки алгоритма WARP (Weighted Approximate Rank Pairwise) для обучения и тонкой настройки моделей с использованием комбинации предварительно обученных языковых моделей и моделей наград. Основные компоненты проекта включают подготовку данных, обучение модели наград, реализацию алгоритма WARP и сравнение производительности моделей.

### Структура

  - `compare_models.py`: Скрипт для сравнения производительности тонко настроенной модели (`theta_sft`) и модели, выровненной с помощью WARP, путем вычисления средней награды и KL-дивергенции.
  - `config.yaml`: Файл конфигурации, содержащий параметры для обучения, пути к моделям и гиперпараметры.
  - `dataset_utils.py`: Утилиты для подготовки данных, включая генерацию наборов данных с промптами из данных IMDb.
  - `reward.py`: Скрипт для обучения модели наград с использованием пар положительных и отрицательных комментариев из набора данных IMDb.
  - `run_exp.py`: Скрипт для запуска экспериментов с изменением гиперпараметров и оценки производительности моделей.
  - `warp.py`: Реализация алгоритма WARP для обучения и тонкой настройки моделей.
  - `README.md`: Документация и инструкции для проекта.
  - `report.md`: Файл отчета для документирования результатов экспериментов и выводов.
  - `requirements.txt`: Список зависимостей, необходимых для работы проекта.

### Настройка

1. **Клонирование репозитория**:
   ```bash
   git clone <repository-url>
   cd <repository-directory>
   ```

2. **Создание виртуального окружения и установка зависимостей**:
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   pip3 install -r requirements.txt
   ```

3. **Настройка путей и параметров**:
   - Отредактируйте `config.yaml`, чтобы установить соответствующие пути для предварительно обученных моделей, наборов данных и гиперпараметров.

### Скрипты и использование

#### 1. Обучение модели наград

Этот скрипт обучает модель наград с использованием пар положительных и отрицательных комментариев из набора данных IMDb.

```bash
python3 reward.py
```

- **Входные данные**: Параметры конфигурации из `config.yaml`.
- **Выходные данные**: Обученная модель наград, сохраненная в указанной директории.

#### 2. Реализация алгоритма WARP

Этот скрипт запускает алгоритм WARP для тонкой настройки предварительно обученной модели (`theta_sft`) и ее выравнивания с использованием модели наград.

```bash
python3 warp.py
```

- **Входные данные**: Параметры конфигурации из `config.yaml`.
- **Выходные данные**: Итоговые веса выровненной модели, сохраненные в `final_weights.pth`.

#### 3. Сравнение моделей

Этот скрипт сравнивает тонко настроенную модель (`theta_sft`) и выровненную модель путем вычисления средней награды и KL-дивергенции.

```bash
python3 compare_models.py
```

- **Входные данные**: Параметры конфигурации из `config.yaml` и сгенерированные модели.
- **Выходные данные**: Значения средней награды и KL-дивергенции.

#### 4. Запуск экспериментов

Этот скрипт проводит эксперименты с изменением выбранного гиперпараметра и оценкой производительности модели.

```bash
python3 run_exp.py
```

- **Входные данные**: Параметры конфигурации из `config.yaml`.
- **Выходные данные**: Графики и логи результатов экспериментов.

### Конфигурация

Файл `config.yaml` содержит все параметры конфигурации, необходимые для выполнения задач проекта. Убедитесь, что все пути к моделям и наборам данных правильно указаны, и установлены нужные гиперпараметры для обучения и экспериментов.