# Отчет
Реализовал алгоритм выравнивания языковых моделей WARP from scratch

# Проделанная работа

В соответствии с условием тестового задания был реализован и протестирован алгоритм выравнивания Warp
в контексте этого были решены такие задачи как:
- обучение модели реварда на отзывах imdb
- реализация пайплайна обучения warp
- реализация пайплайна экспириментов warp

# Эксперименты
К сожалению, не успел позапускать
Однако исходя из написанного в статье можно сказать, что изменение параметра η влияет на баланс между ревардом и KL дивергенцией в модели.
Это позволяет регулировать трейдофф между высокой наградой и низкой KL дивергенцией.
Высокие значения η (около 1):
- Обеспечивают высокие награды
- Приводят к высокой KL
- Подразумевают, что модель сильно отклоняется от начальной инициализации, сохраняя больше новых знаний и поведения, приобретенных в процессе обучения

Низкие значения η (около 0):
- Обеспечивают низкую KL
- Приводят к более низким наградам
- Модель остается ближе к начальной инициализации, что помогает сохранять общее знание и уменьшать риск забывания фундаментальных знаний

Таким образом, выбирая значение η, можно контролировать компромисс между сохранением начальных знаний и адаптацией модели к новым данным и задачам
# Вывод

Поздно спохватился, поэтому не удалось написать оптимальный код, который бы работал быстро, и не удалось погонять модель на разных параметрах и как следствие сделать более глубокие выводы